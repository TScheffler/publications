\documentclass[citeauthoryear]{llncs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{makeidx}  % allows for indexgeneration
\usepackage{paralist} % for inparaenum
\usepackage{multirow} % for multirow
\usepackage{tabularx} % for centering in table
\usepackage{url}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\totaloov}{\% of total OOVs}
\newcommand{\uniqoov}{\% of unique OOVs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\pagestyle{headings}  % switches on printing of running heads
\title{MT @HanBaldwin: Fightin OOVs in German \#twitter}
\titlerunning{Makn Sens a Germa \#twitter}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Uladzimir Sidarenka \and Tatjana Scheffler \and Manfred Stede}
%
\authorrunning{Sidarenka et al.} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Uladzimir Sidarenka, Tatjana Scheffler, Manfred Stede}
%
\institute{University of Potsdam,\\
  \email{{uladzimir.sidarenka@uni-potsdam.de,
      tatjana.scheffler@uni-potsdam.de,
      manfred.stede@uni-potsdam.de},\\ WWW home page:
    \texttt{http://www.ling.uni-potsdam.de/acl-lab/SocMedia/main.htm}}}

\maketitle              % typeset the title of the contribution

\begin{abstract}
  %% Since its launch in March 2006 and until today Twitter constantly
  %% gained more and more popularity as a communication means among the
  %% Internet users. As of the end of 2012, approximately half a billion
  %% messages were published every day by using its services (Terdiman,
  %% \cite{terdiman}).  But, unfortunately, this abundant resource of
  %% textual information can not be exploited to the full extent by
  %% solely relying on standard NLP frameworks, because misspellings,
  %% active usage of slang, and various other aspects of texting language
  %% significantly decrease the chances of successful automatic analysis
  %% of this kind of data.

  This article gives an overview of existing approaches to the problem
  of out-of-vocabulary (OOV) tokens and noisiness phenomena in natural
  language texts. These approaches are classified with regard to the
  size of text spans and knowledge inference mechanisms which they
  rely on in their work. Additionally to that, we conduct quantitative
  and qualitative analyses of unknown words in German Twitter
  messages, in order to see how relevant the OOV and text
  normalization problems are for this particular kind of micro-texts
  and what the characteristics of these OOVs are. In a concluding
  step, we present a set of ad-hoc techniques which are supposed to
  tackle some of the most prominent disturbing effects found during
  these analyses and show how this set of techniques helps us lower
  the average rate of out-of-vocabulary tokens in Twitter messages and
  how this lower OOV-rate in turn helps improve the quality of
  automatic part-of-speech tagging.

  \keywords{twitter, social media, text normalization, spelling
    correction}
\end{abstract}
%
\section{Introduction}
When Jack Dorsey, the present CEO of Twitter Inc., was sending the
very first tweet on March 21, 2006 (Dorsey, \cite{dorsey}), he
probably could not imagine that a few years later presidents and
government officials would use this service to communicate with their
voters and the Pope would be posting short messages holding an iPad in
his hand (Pianigiani, \cite{nyt:pope}). Yet another thing that Jack
Dorsey was apparently not aware of at that moment, was the fact that
his message -- ``just setting up my twttr'' -- already contained a
word which was unknown to the majority of NLP applications existing at
that time, and that there would be many of such words in future
causing a lot of headache to natural language specialists.

Though the problem of out-of-vocabulary words and its closely related
task of textual normalization have been extensively studied in
computational linguistics since as early as the late 1950s
(cf. Petersen, \cite{petersen}) and were certainly anything but new at
the time when mobile communication emerged, it were small messages
that revived interest in this research area in the past two decades.

%% Starting from the second half of the 1990s, not only the number of
%% publications on automatic text correction gradually increased every
%% year but also the focus of those researches steadily turned away from
%% normalization of manually typed or OCR processed official documents to
%% the processing of sloppily created user messages and online chat
%% posts. But even despite this increased interest, most of the
%% researches still were dealing with only English text data. A few
%% exceptions from that are works on French (Beaufort et al.,
%% \cite{beaufort}) and Spanish (Oliva et al., \cite{oliva}).

%% One of the first scientific milestones which marked the renaissance
%% of OOV studies in CL was a comprehensive article by Karen Kukich
%% published in the renowned ACM journal on December 4-th 1992
%% \cite{kukich}. By an interesting coincidence, just exactly one day
%% before that, a British engineer called Neil Papworth had sent the
%% world's first ever SMS-message \cite{guardian:sms}. But in contrast
%% to Papworth's SMS which only said ``Merry Christmas'' to one of his
%% friends, Kukich's work comprised more than 60 pages and provided a
%% fully-fledged review of the state of the art techniques for dealing
%% with unknown words within the scope of spelling correction
%% programs.

%% In this article we are going to provide some more insight about the
%% relevance of OOV issues for German language by showing how many
%% out-of-vocabulary tokens are found in German online texts on average
%% and what kind of linguistic phenomena are main sources of these
%% OOVs. The next Section will first give a short overview and provide a
%% classification of recent scientific approaches to the problem of
%% out-of-vocabulary words in non-standard texts. After that, in Section
%% \ref{error:analysis} we will analyze which types of noisiness
%% phenomena are especially common in German Twitter. Section
%% \ref{normalization} will subsequently describe an automatic procedure
%% for mitigating some of the most prominent of those effects. In a
%% concluding step, we will perform an evaluation of the results of this
%% procedure and give some further suggestions for future research.

In the next Section we will give a short overview of recent scientific
approaches to the problem of tackling out-of-vocabulary words in
non-standard texts. After that, in Section \ref{error:analysis} we
will analyze which types of noisiness phenomena are especially
characteristic for German Twitter. Section \ref{normalization} will
subsequently describe an automatic procedure for mitigating some of
the most prominent of those effects. In a concluding step, we will
perform an evaluation of the results of this procedure and give
further suggestions for future research.

\section{Related Work}

Before proceeding with the description of existing methods for noisy
text normalization (NTN), we first would like to define the criteria
by which these methods could be classified. It should be noted that
there already exist several classifications of NTN approaches
including Kukich (\cite{kukich}), Kobus et al. (\cite{kobus}), and
Sproat et al. (\cite{sproat}).
%% But all of these classifications seem to share one significant common
%% shortcoming, namely they try to involve several independent criteria
%% within one classification scheme.

Kukich (\cite{kukich}), for example, divided NTN techniques into six
classes:
\begin{enumerate}
\item minimum edit distance techniques;
\item similarity key techniques;
\item rule-based techniques;
\item \textit{n}-gram based techniques;
\item probabilistic techniques;
\item neural nets.
\end{enumerate}
Kobus (\cite{kobus}), on the contrary, referred to NTN methods as
\textit{metaphors} and split them into the following three groups:
\begin{enumerate}
\item ``spell checking'' metaphor;
\item ``translation'' metaphor;
\item ``speech recognition'' metaphor.
\end{enumerate}

Though both divisions seem to be justified to some extent, it is
difficult to determine using them whether an NTN approach that detects
and restores incorrectly spelled words on the basis of phonetical
\textit{n}-gram statistics should fall into the \textit{n}-gram based,
probabilistic, spell checking or speech recognition class.

The reason for this confusion is the fact that the above
classifications both rely on several independent criteria at the same
time, though each of these criteria characterizes an NTN system from a
different point of view. As a consequence, unambiguous assignment of
an NTN approach to one particular class often becomes impossible. In
order to avoid this, we instead suggest using separate classifications
for each type of involved criteria which might characterize an NTN
technique.

One of such criteria which in our opinion would be worth a separate
classification is \emph{segmentation level} that is used
\begin{inparaenum}[\itshape a\upshape)]
\item to infer in-vocabulary (IV) equivalents for OOV
  tokens\label{candidate:inferring} and
\item to choose the most probable variant\label{candidate:selection}
  among multiple possible suggestions\footnote{For the cases, when
    segments of different lengths are used for tasks
    \ref{candidate:inferring} and \ref{candidate:selection}, we note
    it explicitly in our classification on which segmentation length
    each of these subtasks relies.}.
\end{inparaenum} For this criterion, we propose division into the following
classes:
\begin{enumerate}
  \item graphematic\footnote{Depending on whether phonetical
    information is involved or not at this level, this class could be
    further divided into a phonographematic and purely graphematic
    subclasses.};
  \item lexical;
  \item phrasal.
\end{enumerate}
Each broader level of this hierarchy is supposed to either incorporate
or ignore information provided by its narrower subsegments. In this
way, we only need to mention one (the broadest) hierarchical class for
the cases when multiple segmentation levels are involved by some
techniques.

The second criterion regards the \emph{type of information induction}
that is used to devise the correction rules. This leads us to the
usual NLP-taxonomy which divides all approaches into:
\begin{enumerate}
  \item rule-based;
  \item statistical\footnote{Depending on the type of training data
    required, this class is in turn usually divided into unsupervised,
    semi-supervised, and supervised groups.};
  \item and hybrid ones.
\end{enumerate}

With these two classifications, we will now try to present and group
together approaches to the NTN task which appeared in the literature
in recent years.

The earlier works on NTN commonly relied on either purely graphematic
or phonographematic levels of segmentation for deriving normalization
variants of incorrectly spelled words. To purely graphematic systems
belong methods suggested by Brill and Moore (\cite{brill}), Sproat et
al. (\cite{sproat}), and Clark (\cite{alexander-clark}). As
phonographematic approaches one could regard works done by Toutanova
and Moore (\cite{toutanova}), Choudhury et al. (\cite{choudhury}),
Cook and Stevenson (\cite{cook}) etc. With regard to the type of
information inference, practically all of these methods were
supervised with the exception of Cook and Stevenson (\cite{cook}) who
claimed to use an unsupervised technique.

Starting from the second half of the 2000s, the raising influence and
improved quality of machine translation tools lead to the development
of NTN technologies which used broader levels of segmentation. In
\cite{aw}, Aw et al. suggested a supervised statistical system for
normalization of SMS-messages which operated on automatically aligned
phrases A few years later, Clark and Araki (\cite{clark-araki})
described a purely rule-based method which used mappings of
non-standard words and phrases to their corresponding standard
language forms.

As noted by Kobus et al. (\cite{kobus}), NTN methods relying on either
graphematic or phrasal segments usually revealed complementary
strengths and weaknesses. This notion led NLP scientists to the idea
that by incorporating multiple levels of the language into one NTN
system the total performace of the whole system would improve as
different sources of information would benefit from each other. As a
consequence of this, a wealth of combined techniques emerged in the
past few years. Among these we should especially mention works by
Kobus et al. (\cite{kobus}), Kaufmann (\cite{kaufmann}), Han and
Baldwin (\cite{han}), and Oliva et al. (\cite{oliva}). The majority of
these systems used the whole range of segmentation levels from
phonographematatic to phrasal one, and in many cases they also applied
different knowledge inference mechanisms to different levels of the
language.

%% Eventually, in \cite{han}, an article called ``Lexical Normalization
%% of Short Text Messages: Makn Sens a \#{}twitter'' was published by Han
%% and Baldwin. In this article the authors separated the tasks of
%% identification of ill-formed words and finding appropriate correction
%% for them. For the former problem, they first generated a confusion set
%% (CS) for each word unknown to \texttt{GNU aspell}. Based on this set,
%% the decision was made whether a particular word had to be corrected or
%% regarded as vlid. Subsequently, for words identified as ill-formed the
%% most probable restoration candidate was chosen from CS by combining
%% features resulting from dictionary lookup, analysis of surrounding
%% context, and estimating word similarity to each proposed correction
%% variant. According to authors' estimations, this combination allowed
%% them to outperform most of the NTN methods existing at that time.

It should however be noted that almost all of the above methods mainly
concentrated on only English data. A few exceptions from that are
approaches suggested by Beaufort et al. (\cite{beaufort}) for French
and Oliva et al. (\cite{oliva}) for Spanish. To find out which
peculiarities of ill-formed words are characteristic for German, we
will perform a quantitive and qualitative analysis of unknown words in
German Twitter in the next Section in order to see what kind of NTN
techniques would be more suitable for handling ill-formed words there.

\section{Analysis of Unknown Tokens}\label{error:analysis}

In order to estimate the percentage of unknown words in Twitter
messages, we randomly selected 10,000 tweets from a previously
collected corpus, split them into sentences and tokenized using social
media-aware tokenizer by Christopher Potts
\footnote{\url{http://sentiment.christopherpotts.net/code-data/happyfuntokenizing.py}}.
After skipping all words which did not contain any alphabetic
characters or consisted only of a single letter, we obtained a list of
129,146 tokens. As reference systems for dictionary lookup we used
open source spell checking program \texttt{hunspell}\footnote{Ispell
  Version 3.2.06 (Hunspell Version 1.3.2); dictionary de\_DE.} and
publicly available part-of-speech tagger
\texttt{TreeTagger}\footnote{Version 3.2 with German parameter file
  UTF-8.} (Schmid, \cite{schmid}).

Out of this token list, 26,018 tokens (20.15~\%) were regarded as
unknown by \texttt{hunspell} and 28,389 tokens (21.98~\%) were
considered as OOV by \texttt{TreeTagger}. We also performed similar
estimations after leaving only unique words without taking into
account their frequencies. This allowed us to shrink our initial token
list by four times to 32,538 unique tokens. The relative rate of
unknown words raised as expected and run up to 46.96~\% for
\texttt{hunspell} and 58.24~\% for \texttt{TreeTagger}.

Here once again the question of classification arose. This time with
regard to the reasons, why different tokens could have been omitted
from corresponding applications' dictionaries. In this respect,
division into following groups seemed to us to be appropriate:
\begin{enumerate}
  \item \textbf{Objective limitedness of machine-readable dictionary
    (MRD)}. Among this group, we counted words of basic vocabulary
    which did not get into applications' MRD either because they
    supposedly were rare or because they did not exist at the time
    when dictionaries were created. Another reason for the inclusion
    in this type was the belonging of a word to an open lexical or
    part-of-speech class (like, for example, named entities or
    interjections) which are often omitted from MRDs due to
    the impossibility to fully cover these classes;\label{dict}
  \item \textbf{Sloppiness of users' input}. In the scope of this
    second group, we considered incorrect spellings of words
    encountered in text;\label{spell}
  \item \textbf{Stylistic specifics of text genre}. This group
    comprised words which could be considered as illegal from the
    point of view of standard language but were perfectly valid terms
    in the domain of web discourse or more specifically in Twitter
    communication.\label{style}
\end{enumerate}

In order to see how detected out-of-vocabulary words were distributed
among and within these 3 major groups, we manually analyzed all OOV
tokens which appeared in text more than once and also looked at 1,000
randomly selected hapax legomena. The results of these estimations are
shown and explained below.

We subdivided the class \ref{dict} into the following subclasses:
\begin{enumerate}
\item regular German words, e.g. \textit{aufm},
  \textit{losziehen};\label{regular}
\item compounds, e.g.  \textit{Altwein},
  \textit{Amtsapothekerin};\label{compound}
\item abbreviations, e.g. \textit{NBG}, \textit{OL};\label{abbr}
\item interjections, e.g.  \textit{aja}, \textit{haha};\label{inj}
\item named entities, with subclasses:\label{ne}
  \begin{enumerate}
  \item persons, e.g.  \textit{Ahmadinedschad}, \textit{Schweiger};
  \item geographic locations, e.g.  \textit{Biel}, \textit{Limmat};
  \item companies, e.g. \textit{Apple}, \textit{Facebook};
  \item product names, e.g. \textit{iPhone}, \textit{MacBook};
  \end{enumerate}
\item neologisms, with subclasses:\label{neolog}
  \begin{enumerate}
    \item newly coined German terms, e.g. \textit{entfolgen},
      \textit{gegoogelt};\label{new}
    \item loanwords, e.g. \textit{Community},
      \textit{Stream};\label{loan}
  \end{enumerate}
\item and, finally, foreign words like \textit{is} or \textit{now}
  which in contrast to \ref{loan} were not mentioned in any existing
  German lexica and did not comply with inflectional rules of German
  grammar.\label{fw}
\end{enumerate}
Though this division is admittedly arbitrarily to a certain degree and
also has the disadvantage of simultaneously involving different
linguistic criteria, the underlying notion here was simple -- valid
words could have been omitted from an MRD either due to the
limitations of developers' capacities (group \ref{regular}), active
word formation processes or lexical productivity of the language
itself (groups \ref{compound} through \ref{new}) or also due to
language's openness to foreign language systems (groups \ref{loan} and
\ref{fw}).

In Table \ref{table:mrd}, percentage figures for each of the above
subgroups are shown. We have considered OOV-distributions for both
\texttt{hunspell} and \texttt{TreeTagger}. For each of them, we
estimated the percentage of a particular subclass with regard to the
total number of occurrences of OOV-tokens (column ``\totaloov{}'') as
well as with regard to their percentage rate in the list of only
unique OOVs disregarding their frequencies (column ``\uniqoov{}'').
\begin{table}
  \caption{Distribution of OOV words belonging to the class
    ``Objective limitedness of MRD''\label{table:mrd}}
  \begin{tabular}{p{0.4\textwidth}*{4}{>{\centering\arraybackslash}p{0.15\textwidth}}}
    \hline\noalign{\smallskip}
    \multirow{2}{*}{OOV subclass} & %
    \multicolumn{2}{c}{\texttt{hunspell}} & %
    \multicolumn{2}{c}{\texttt{TreeTagger}}\\
    & \totaloov{} & \uniqoov{} & \totaloov{} & \uniqoov{}\\
    \noalign{\smallskip} \hline
    regular German words & 7.82 & 8.78 & 2.8 & 3.49\\
    compounds & 1.21 & 2.42 & 2.51 & 4.55\\
    abbreviations & 3.98 & 4.77 & 3.27 & 3.44\\
    interjections & 5.95 & 4.54 & 5.58 & 4.29\\
    person names & 4.73 & 6.41 & 2.32 & 3.47\\
    geographic locations & 1.5 & 2.53 & 1.16 & 1.88\\
    company names & 2.27 & 2.84 & 4.35 & 3.01\\
    product names & 2.13 & 2.57 & 2.45 & 3.23\\
    newly coined terms & 1.35 & 1.31 & 3.33 & 2.38\\
    loanwords & 3.68 & 4.03 & 3.29 & 2.87\\
    foreign words & 11.5 & 13.76 &  9.57 & 10.91\\\hline
    {\bfseries total} & 46.12 & 53.96 & 40.63 & 43.52\\
    \noalign{\smallskip} \hline
  \end{tabular}
\end{table}

Similarly to the class \ref{dict}, we subdivided the group
``Sloppiness of users' input'' into the following subgroups:
\begin{enumerate}
  \item insertions, e.g. \textit{dennen} instead of \textit{denen};
  \item deletions, e.g. \textit{scho} instead of \textit{schon};
  \item substitutions, e.g. \textit{fur} instead of \textit{f\"ur};
\end{enumerate}
according to the type of operation which led to a particular spelling
mistake. In cases when multiple different operations were involved
simultaneously, we explicitly marked each of these operations in our
data. Statistical distribution of these subclasses is shown in Table
\ref{table:spell}.
\begin{table}
  \caption{Distribution of OOV words belonging to the class
    ``Sloppiness of users' input''\label{table:spell}}
  \begin{tabular}{p{0.4\textwidth}*{4}{>{\centering\arraybackslash}p{0.15\textwidth}}}
    \hline\noalign{\smallskip}
    \multirow{2}{*}{OOV subclass} & %
    \multicolumn{2}{c}{\texttt{hunspell}} & %
    \multicolumn{2}{c}{\texttt{TreeTagger}}\\
    & \totaloov{} & \uniqoov{} & \totaloov{} & \uniqoov{}\\
    \noalign{\smallskip} \hline
    insertions & 0.49 & 1 & 0.18 & 0.34 \\
    deletions & 8.44 & 6.38 & 6.52 & 5.27 \\
    substitutions & 2.17 & 3.37 & 1.11 & 1.2 \\\hline
    {\bfseries total} & 11.1 & 10.75 & 7.81 & 6.81\\
    \noalign{\smallskip} \hline
  \end{tabular}
\end{table}

As is clear from the table, deletions are by far the most common type
of misspellings. The reasons for that are on the one hand either
relatively frequent omissions of characters made by user or even more
often automatic truncations of too long messages which were performed
by Twitter service itself.\footnote{As is generally known, Twitter
  imposes a strong restriction on the length of posted messages which
  can be no longer than 140 characters. Upon exceeding this length,
  tweets get automatically truncated to the maximal allowed length.}

Finally, the third group -- ``Stylistic specifics of text genre'' --
was subdivided into the subgroups:
\begin{enumerate}
  \item hashtags, e.g. \textit{\#Kleinanzeigen};
  \item @-tokens, e.g. \textit{@ZDFonline};
  \item links, e.g. \textit{http://t.co};
  \item smileys, e.g. \textit{:-P};
  \item slang, e.g. \textit{OMG}.
\end{enumerate}
according to the lexical or syntactic class which these tokens
belonged to. Apart from lexical expressions pertaining to the genre of
Internet-based communication, we also marked as \textit{slang}
spelling variants of standard language words which could be regarded
as their corresponding colloquial equivalents. As such equivalents, we
considred spellings like \textit{ned} instead of \textit{nicht} or
\textit{grad} instead of \textit{gerade}. These cases were marked by
us both as misspellings and as slang. A detailed statistics on the
above subgroups of class 3 is shown in Table \ref{table:style}:
\begin{table}
  \caption{Distribution of OOV words belonging to the class
    ``Stylistic specifics of text genre''\label{table:style}}
  \begin{tabular}{p{0.4\textwidth}*{4}{>{\centering\arraybackslash}p{0.15\textwidth}}}
    \hline\noalign{\smallskip}
    \multirow{2}{*}{OOV subclass} & %
    \multicolumn{2}{c}{\texttt{hunspell}} & %
    \multicolumn{2}{c}{\texttt{TreeTagger}}\\
    & \totaloov{} & \uniqoov{} & \totaloov{} & \uniqoov{}\\
    \noalign{\smallskip} \hline
    hashtags & 7.35 & 6.18 & 13.06 & 10.59\\
    @-tokens & 13.02 & 20.23 & 16.19 & 21.91\\
    links & 2.43 & 0.4 & 4.89 & 6.07\\
    smileys & 2 & 0.73 & 6.88 & 1.2\\
    slang & 16.22 & 5.27 & 6.94 & 4.77\\\hline
    {\bfseries total} & 41.02 & 32.81 & 47.96 & 44.54\\
    \noalign{\smallskip} \hline
  \end{tabular}
\end{table}

A striking outlier of 16.22~\% for slang in column 1 of the table is
due to the fact that the word ``RT'' which occurred 1,235 and was by
far the most frequent OOV in our data was recognized as
out-of-vocabulary by \texttt{hunspell} but not recognized as such by
\texttt{TreeTagger}.

This knowledge about composition of unknown and ill-formed words
should now help us select or devise techniques for recognizing and
correcting such tokens in cases where it is needed. These procedures
are described in the next Section of the article.

\section{Text Normalization Procedure}\label{normalization}
\subsection{Replacement of Twitter-Specific Phenomena}
%
\subsection{Restoration of Umlauts}
%
\subsection{Squeezing of Elongated Words}
%
\subsection{Translation of Slang Idioms}
%
\section{Evaluation}\label{evaluation}


\section{Conclusion}\label{concl}

%% This article provided an overview of exisiting approaches to noisy
%% text normalization task. Additionally, all mentioned methods were
%% classified on the basis of two independent criteria. In section
%% \ref{error:analysis}, we performed qulitative and quantitive analyses
%% of out-of-vocabulary words in German tweets and suggested a set of
%% ad-hoc techniques for mitigating their potential negative influence on
%% natural language processing. This procedure allowed us to reduce the
%% total OOV rate by ... \% for \texttt{hunspell} and by ... \% for
%% \texttt{TreeTagger}.

%% Nevertheless, we should honestly admit that our system still has
%% potential for development and research, since it mainly addresses only
%% one of three main groups of OOV tokens. Future directions should
%% certainly include a more thorough tackling of unintentional spelling
%% mistakes and especially their most prominent types -- deletions and
%% substitutions.

%% Furthemore, a better evalution technique as well as comparison with
%% other systems are needed for our normalization procedure. On the one
%% hand, an \textit{extrinsic} evaluation should be performed (cf. Sparck
%% Jones and Galliers, \cite{sparck}) which means that we not only have
%% to show how the rate of OOV words goes down but much more how this
%% lower OOV rate affects the work of the whole NLP system. On the other
%% hand, we need to assess the quality of our procedure's work on the
%% basis of metrics used by other researchers.

%% One possible estimation criterion which was used by Aw et
%% al. (\cite{aw}), Kaufmann (\cite{kaufmann}), Beaufort et
%% al. (\cite{beaufort}), and Oliva et al. (\cite{oliva}) is the BLEU
%% score (Papineni et al., \cite{papineni}). Another possibility would be
%% to use the Word (WER) and Sentence Error Rates (SER) as suggested by
%% Kobus et al. (\cite{kobus}). However, an obvious difficulty that we
%% already encountered here is that both metrics highly rely on a
%% subjective notion of the look of a ``normalized'' message. While the
%% BLEU score could be an approriate criterion for normalization of
%% messages like ``i luv ma mather and wd do evrythin 4 her'' which in
%% fact looks like as if it were not English. But such highly distorted
%% tweets are rather atypical for German. In this regard, WER and SER
%% could be considered as more appropriate measurement criteria. But
%% these metrics once again are rather dealing with spelling mistakes and
%% would highly depend on whether one, for example, would consider the
%% hash sign in a hashtag as an error.

\subsection*{\ackname}

This work was financially supported by ... as part of collaborative
project ``...''. The authors are also thankful to ... for help with
the analysis of data.

%
% ---- Bibliography ----
%
\bibliographystyle{splncs}
\begin{thebibliography}{}
\bibitem[2006]{aw}
  Aw, A., Zhang, M., Xiao, J., Su, J.: %
  A Phrase-based Statistical Model for {SMS} Text Normalization. %
  COLING/ACL (2006) 33--40 %

\bibitem[2002]{bangalore}
  Bangalore, S., Murdock, V., Riccardi, G.: %
  Bootstrapping Bilingual Data using Consensus Translation for a %
  Multilingual Instant Messaging System. %
  COLING (2002) 33--40 %

\bibitem [2010]{beaufort}
  Beaufort, R., Roekhaut, S., Cougnon, L. A.,  Fairon, C.: %
  A Hybrid Rule/Model-Based Finite-State Framework for %
  Normalizing SMS Messages. %
  ACL (2010) 770--779 %

\bibitem[2000]{brill}
  Brill, E., Moore, R. C.: %
  An improved model for noisy channel spelling correction. %
  ACL %
  (2000) 286--293 %

\bibitem[2003]{alexander-clark}
  Clark, A.: %
  Pre-processing very noisy text. %
  In Proceedings of Workshop on Shallow Processing of Large Corpora. %
  (2003) 12--22

\bibitem[2007]{choudhury}
  Choudhury, M., Saraf, R., Jain, V., Mukherjee, A., Sarkar, S., Basu %
  A.: %
  Investigation and Modeling of the Structure of Texting Language. %
  International Journal of Document Analysis and Retrieval: Special %
  Issue on Analytics of Noisy Text. {\bfseries 10} %
  (2007) 157--174 %

\bibitem[2011]{clark-araki}
  Clark, E., Araki, K.: %
  Text Normalization in Social Media: Progress, Problems and %
  Applications for a Pre-processing System of Casual English. %
  PACLING. Procedia - Social and Behavioral Sciences {\bfseries 27} %
  (2011) 2--11

\bibitem[2009]{cook}
  Cook, P., Stevenson, S.: %
  An unsupervised model for text message normalization, %
  Proceedings of the Workshop on Computational Approaches %
  to Linguistic Creativity. %
  CALC '09. (2009) 71--78

\bibitem[2006]{dorsey}
  Dorsey, J.: %
  "just setting up my twttr". %
  \url{https://twitter.com/jack/status/20} %
  Accessed February 26, 2013. (2006)

\bibitem[2011]{han}
  Han, B., Baldwin, T.: %
  Lexical Normalization of Short Text Messages: Makn Sens %
  a \#{}twitter. %
  ACL HLT. %
  (2011) 368--378

\bibitem[2008]{jurafsky}
  Jurafsky, D., Martin, J. H.: %
  Speech and Language Processing. 2nd Edition. %
  Prentice Hall (2008) 129, 323

\bibitem [2010]{kaufmann}
  Kaufmann, M.: %
  Syntactic normalization of twitter messages. %
  The 8-th International Conference on Natural Language Processing. %
  (2010)

\bibitem [2008]{kobus}
  Kobus, C., Yvon, F., Damnati, G.: %
  Normalizing {SMS}: are Two Metaphors Better than One? %
  COLING (2008) 441--448

\bibitem[1992]{kukich}
  Kukich, K.: %
  Techniques for Automatically Correcting Words in Text. %
  ACM Computing Surveys {\bfseries 24/4} (1992) 378--439

\bibitem[2012]{guardian:sms}
  McVeigh, T.: %
  Text messaging turns 20. %
  The Observer (December 1, 2012)

\bibitem[2012]{mukherjee}
  Mukherjee, S., Malu, A., Balamurali, A. R., Bhattacharyya, P.: %
  TwiSent: A Multistage System for Analyzing Sentiment in Twitter. %
  In Proceedings of The 21st ACM Conference on Information and %
  Knowledge Management CIKM 2012, Hawai, (Oct 29 - Nov 2, 2012)


  McVeigh, T.: %
  Text messaging turns 20. %
  The Observer (December 1, 2012)

\bibitem [2013]{oliva}
  Oliva, J., Serrano, J. I., and Del Castillo, M. D., %
  and Igesias, Á.: %
  A {SMS} normalization system integrating multiple %
  grammatical resources. %
  Natural Language Engineering. %
  (2013) 121--141 %

\bibitem [2002]{papineni}
  Papineni, K., Roukos, S., Ward, T., Zhu, W.-J.: %
  Bleu: a Method for Automatic Evaluation of Machine Translation. %
  ACL (2002) 311--318

\bibitem [1980]{petersen}
  Petersen, L. J.: %
  Computer Programs for Detecting and Correcting Spelling Errors. %
  Communications of the ACM {\bfseries 23/ 12} (1980) 676--687

\bibitem[2012]{nyt:pope}
  Pianigiani, G., Donadio, R.: %
  Twitter Has A New User: The Pope. %
  The New York Times. Page A6. (December 4, 2012)

\bibitem[2001]{sproat}
  Sproat, R., Black, A. W., Chen, S. F., Kumar, S., %
  Ostendorf, M., Richards, Ch.: %
  Normalization of non-standard words. %
  Computer Speech \& Language, {\bfseries 15/3} (2001) 287--333

\bibitem[1994]{schmid}
  Schmid, H.: %
  Probabilistic Part-of-Speech Tagging Using Decision Trees. %
  In Proceedings of International Conference on New Methods in %
  Language Processing. (1994)

\bibitem[1996]{sparck}
  Sparck Jones, K., Galliers, J. R.: %
  Evaluating Natural Language Processing Systems. %
  An Analysis and Review. Lecture Notes in Computer %
  Science 1083, Springer. (1996)

\bibitem[2012]{terdiman}
  Terdiman, D.: %
  Report: Twitter hits half a billion tweets a day. %
  \url{http://timmurphy.org/2009/07/22/line-spacing-in-latex-documents/},%
  Accessed February 26, 2013. (2012)

\bibitem[2002]{toutanova}
  Toutanova, K., Moore, R. C.: %
  Pronunciation Modeling for Improved Spelling Correction. %
  ACL (2002) 144--151

\end{thebibliography}
%
% ---- End of Bibliography ----
%
\end{document}
